# 🔢 2. 常见的序列任务

---

### ✨ Transformer 架构的数据类型
Transformer 架构主要处理 **序列类型** 的数据，或者可以被转换成序列类型的数据（例如文本、时间序列、音频特征、图像的序列化特征等）。

---

### 🎯 常见的序列任务
1.  **序列标注（Sequence Labeling）**
    为输入序列中的每个元素分配一个标签（例如词性标注、命名实体识别、分词）。

2.  **序列到单值（Sequence to One）**
    将整个输入序列映射为一个单一输出值（例如文本分类、情感分析、句子主题判断）。

3.  **单值到序列（One to Sequence）**
    从一个输入值生成一个输出序列（例如标题生成、根据关键词生成段落）。

4.  **序列到序列（Sequence to Sequence）**
    将一个输入序列转换为另一个输出序列（例如机器翻译、文本摘要、对话生成）。


# 🧮 3.2 RNN 公式

---

### 🔍 公式解读
这张图展示了循环神经网络（RNN）的核心计算过程，用于处理序列数据。

1.  **隐藏状态更新公式**
    $$
    h^{<t>} = \sigma\left(x^{<t>} @ W_x + h^{<t-1>} @ W_h + B_x\right)
    $$
    - \(h^{<t>}\)：第 \(t\) 时刻的隐藏状态，即中间隐藏值。如果计算第一个中间隐藏值，需要手动设置\(h^{<t-1>}\)为0矩阵
    - \(x^{<t>}\)：第 \(t\) 时刻的输入向量。
    - \(W_x, W_h\)：输入和隐藏状态的权重矩阵。前者将x特征值进行特征提取
    - \(B_x\)：偏置项，位移矩阵。
    - \(\sigma\)：激活函数（Sigmoid函数）。
    - 这个公式体现了 RNN 的“循环”特性：当前时刻的隐藏状态由当前输入和上一时刻的隐藏状态共同决定。

2.  **输出计算公式**
    $$
    \hat{y}^{<t>} = \sigma\left(h^{<t>} @ W_y + B_y\right)
    $$
    - \(\hat{y}^{<t>}\)：第 \(t\) 时刻的预测输出。
    - \(W_y\)：隐藏状态到输出的权重矩阵。
    - \(B_y\)：输出层偏置项。
    - 该公式将隐藏状态映射为最终的输出，例如在序列标注任务中为每个词分配标签。

---

# ⚠️ RNN 的局限性
1.  **严格顺序计算**：无法并行处理，计算效率低
2.  **长距离依赖削弱**：时间步间隔越远，早期信息权重越低

正是这些局限性，催生了后续的 Transformer 架构。

# 🎯 3.3 注意力机制（Attention）

---

### 🧮 核心公式
注意力机制通过为输入序列的每个元素分配动态权重，解决了 RNN 长距离依赖的问题。

1.  **值向量计算**
    $$
    v^{<t>} = x^{<t>} @ W_v \quad \Rightarrow \quad V
    $$
    - 将每个时间步的输入向量 \(x^{<t>}\) （原始数据）映射为值向量 \(v^{<t>}\)，即特征提取，所有值向量构成值矩阵 \(V\)。

2.  **隐藏状态计算**
    $$
    h^{<t>} = \sigma\left(w_{score}^{<t>} @ \boldsymbol{V} + B_x\right)
    $$
    - \(w_{score}^{<t>}\) 是第 \(t\) 时刻的注意力得分向量，特征值是个向量，t是向量中第t个值，与值矩阵 \(V\) 相乘得到加权后的信息。
    - 该公式体现了注意力的核心思想：当前时刻的隐藏状态由所有输入位置的信息加权求和得到，而非仅依赖前一时刻的隐藏状态。
    - 三个特征值并行，不相互依赖，可以一次性得到所有特征值

3.  **输出计算**
    $$
    \hat{y}^{<t>} = \sigma\left(h^{<t>} @ W_y + B_y\right)
    $$
    - 将包含注意力加权信息的隐藏状态线性+激活操作。
# ⚖️ 3.4 注意力权重 \(w_{score}^{<t>}\)

---

### 🧮 核心公式
为了更合理地计算注意力权重，我们引入查询（Query）和键（Key）的机制，避免直接使用输入自身进行计算。

1.  **查询向量计算**
    $$
    q^{<t>} = x^{<t>} @ W_q
    $$
    - 从当前时间步的输入 \(x^{<t>}\) 中提取查询向量 \(q^{<t>}\)，得到当前元素Query值。
 

2.  **键向量计算**
    $$
    k^{<t>} = x^{<t>} @ W_k \quad \Rightarrow \quad K
    $$
    - 从每个时间步的输入 \(x^{<t>}\) 中提取键向量 \(k^{<t>}\)，得到对于当前元素而言，其它位置的key值，所有键向量构成键矩阵 \(K\)”。

3.  **注意力得分计算**
    $$
    w_{score}^{<t>} = q^{<t>} @ K^T
    $$
    - 通过计算查询向量与所有键向量的相似度，得到当前时间步对所有输入位置的注意力得分，即权重的原始分数。

---

### 💡 设计思想
- **避免自依赖**：将输入映射为独立的 Query 和 Key 特征，通过二者交互衡量关联强度，而非直接用输入自身计算权重，让依赖关系捕捉更客观。
- **全局关联**：单个时间步的 Query 与所有时间步的 Key 计算相似度，一次性生成全局注意力权重，从根本上解决 RNN 的长距离依赖问题。
# 🔄 3.5 自注意力机制（Self Attention）
### 🧮 核心公式（用自己的Q去匹配别人的K）
1.  生成值矩阵：$v^{<t>} = x^{<t>} @ W_v \Rightarrow V$
2.  生成键矩阵：$k^{<t>} = x^{<t>} @ W_k \Rightarrow K$
3.  生成查询矩阵：$q^{<t>} = x^{<t>} @ W_q \Rightarrow Q$
4.  计算相似度得分：$w_{score}^{<t>} = q^{<t>} @ K^T$
5.  加权聚合信息：$h^{<t>} = \sigma\left(w_{score}^{<t>} @ V + B_x\right)$
6.  生成最终输出：$\hat{y}^{<t>} = \sigma\left(h^{<t>} @ W_y + B_y\right)$
# ⚡ 3.6 并行计算

---

### 🧮 矩阵运算公式
1.  生成值矩阵：$V = X @ W_v$
2.  生成键矩阵：$K = X @ W_k$
3.  生成查询矩阵：$Q = X @ W_q$
4.  计算注意力得分矩阵：$W_{score} = Q @ K^T$
5.  计算隐藏状态矩阵：$H = \sigma\left(W_{score} @ V + B_x\right)$
6.  计算最终输出矩阵：$\hat{Y} = \sigma\left(H @ W_y + B_y\right)$ 
### 🧠 可训练参数
在上述公式中，需要通过训练学习的参数为：$W_v$（值矩阵权重）、$W_k$（键矩阵权重）、$W_q$（查询矩阵权重）、$B_x$（隐藏层偏置）、$W_y$（输出层权重）、$B_y$（输出层偏置）

### 最后计算loss值
# 🎯 4. 序列到单值任务

### 🧮 核心公式
该任务的目标是将整个输入序列（如“跟 达叔 一起 学习 AI”）映射为一个单一的输出值（如“人工智能”），典型应用包括文本分类、情感分析等。

1.  **全局隐藏状态计算**
    $$
    H = \sigma\left(Q @ K^T @ V + B_x\right)
    $$
    - 通过自注意力机制，对输入序列的所有信息进行加权聚合，得到包含全局上下文的隐藏状态矩阵 \(H\)。中间隐藏值
    - 一次性计算所有位置的关联，解决了 RNN 的长距离依赖问题。

2.  **序列到单值输出**
    $$
    \hat{Y} = \sigma\left(\frac{h^{<1>} + \dots + h^{<5>}}{5} @ W_y + B_y\right)
    $$
    - 对所有时间步的隐藏状态 \(h^{<t>}\) 进行平均池化，得到一个代表整个序列的向量。
  
# 🛡️ 5 掩码自注意力（Mask Self Attention）

### 🧮 核心公式
掩码自注意力是标准自注意力的扩展，通过在注意力得分上施加掩码，防止模型在生成当前词时看到未来的词，从而适配文本生成等序列到序列任务。（单值到序列计算）

1.  **计算注意力得分**
    $$
    W_{score} = Q @ K^T
    $$
    - 计算查询矩阵 \(Q\) 与键矩阵 \(K\) 的相似度，得到原始注意力得分矩阵。

2.  **施加掩码**
    $$
    W_{mask} = mask(W_{score})
    $$
    - 对注意力得分矩阵应用掩码，将未来位置的得分置为极小值（如负无穷），使其在后续的 Softmax 计算中权重趋近于 0。
    - 这确保了在生成第 \(t\) 个词时，模型只能看到第 \(t\) 个词及之前的输入，符合文本生成的因果逻辑。

3.  **计算隐藏状态**
    $$
    H = \sigma\left(W_{mask} @ V + B_x\right)
    $$
    - 使用经过掩码的注意力得分对值矩阵 \(V\) 进行加权聚合，得到隐藏状态矩阵。

4.  **计算输出**
    $$
    \hat{Y} = \sigma\left(H @ W_y + B_y\right)
    $$
    - 将隐藏状态通过线性变换和激活函数，生成最终的序列输出。

# ⚙️ 6. 序列到序列

---

### 📥 Encoder（编码器）
**自注意力**
$$H_e = \sigma\left(Q_x @ K_x^T @ V_x + B_x\right)$$
- 对输入序列进行编码，通过自注意力捕捉输入内部的全局依赖关系，生成包含完整上下文信息的隐藏状态。
- 用\(H_e\)计算K,V

---

### 📤 Decoder（解码器）
1.  **掩码自注意力**
    $$H_d = \sigma\left(mask(Q_y @ K_y^T) @ V_y + B_y\right)$$
    - 处理输出序列，通过掩码防止模型看到未来的词，确保生成过程符合因果顺序。
    - 用\(H_d\)计算Q

2.  **交叉注意力**
    $$H = \sigma\left(Q_d @ K_e^T @ V_e + B_{\hat{y}}\right)$$
    - 将解码器的查询与编码器的键、值进行交互，让生成的每个词都能关注到输入序列的全部信息，从而实现翻译、摘要等任务。

# 📚 模型基础编码技术全解析

---

## 8. 分词编码（Tokenizer）字典
### 🔍 如何编码
1.  **切分文本**：将输入句子（如“跟 达叔 一起 学习 AI”）拆分为最小语义单元（Token）。
2.  **分配ID**：为每个Token分配唯一的数字ID（如“跟”→23，“达叔”→2448）。
3.  **生成序列**：输出数字ID序列，作为模型输入。

### 🛠️ 在该模型中的运用
- 作为模型的**输入入口**，将人类文本转换为机器可处理的数字序列。
- 数字ID送入词嵌入层，转化为语义向量。

### ⚠️ 需要注意的问题
- **未知词（UNK）**：词表外的词会被替换为“未知词”ID，可能丢失信息。
- **子词切分**：罕见词会拆分为子词，但切分不当可能破坏语义。
- **词表大小**：词表过大增加计算量，过小则未知词增多，需权衡。

---

## 9. 词嵌入（Embedding）矩阵相乘
### 🔍 是什么
将分词得到的数字ID，映射为固定长度的稠密向量（如512维），向量包含词的语义信息，语义越近的词向量距离越近。

### 🛠️ 在该模型中的运用
- 是模型的**语义理解基础**，将离散ID转化为连续语义向量。
- 词嵌入会被分别投影到查询（Q）、键（K）、值（V）空间，用于自注意力计算。

### ⚠️ 需要注意的问题
- **维度选择**：维度太小无法表达丰富语义，太大则增加计算负担。
- **预训练与微调**：通常使用预训练词嵌入，再在具体任务上微调。
- **多义词问题**：基础词嵌入无法区分多义词，需结合上下文模型解决。

---

## 10. 位置编码（Position Encoding）
### 🔍 为什么需要
自注意力机制本身无序，不关心词序。为让模型理解语序，需给词嵌入显式加入位置信息。

### 🛠️ 如何实现
使用**正弦余弦编码**，通过三角函数计算位置向量，再与词嵌入相加：
- 第 \(t\) 个位置、第 \(i\) 个维度：
  - 若 \(i\) 为偶数：\(p_i^{<t>} = \sin\left(\frac{t}{len^{\frac{i}{embed\_size}}}\right)\)
  - 若 \(i\) 为奇数：\(p_i^{<t>} = \cos\left(\frac{t}{len^{\frac{i-1}{embed\_size}}}\right)\)
- 生成唯一且有规律的位置向量，让模型区分不同位置的词。

### 🛠️ 在该模型中的运用
- 与词嵌入向量相加，得到**语义+位置**向量，作为编码器输入。
- 确保模型理解“我 爱 你”和“你 爱 我”的区别。

# 🧠 多头自注意力机制完整计算公式

### 1. 输入线性投影
将输入向量分别投影到 Query、Key、Value 三个空间：
$$
\begin{align*}
V &= \text{Input} @ W_v \\
Q &= \text{Input} @ W_q \\
K &= \text{Input} @ W_k
\end{align*}
$$
- 输入维度：`(5, 512)`，有5条数据，每条数据提取512个特征
- 权重矩阵 \(W_v, W_q, W_k\) 维度：`(512, 768)`
- 输出 \(V, Q, K\) 维度：`(5, 768)`将512个特征值转换成768个特征值得到单个注意力机制的K、Q、V

---

### 2. 多头拆分与形状变换
将投影后的向量按头数（如 8 头）拆分，得到每个头的子向量：
$$
\begin{align*}
V &= V.\text{reshape}(\text{head}, \text{len}, \frac{\text{hidden}}{\text{head}}) \\
Q &= Q.\text{reshape}(\text{head}, \text{len}, \frac{\text{hidden}}{\text{head}}) \\
K &= K.\text{reshape}(\text{head}, \text{len}, \frac{\text{hidden}}{\text{head}})
\end{align*}
$$
- 头数 `head = 8`，序列长度 `len = 5`，隐藏层维度 `hidden = 768`
- 三位张量：每个头的子向量维度：`(8, 5, 96)`（因为 \(768/8=96\)）8个头（子矩阵）、5个数据量、96个特征）

---

### 3. 计算注意力得分矩阵
在每个头内计算 Query 与 Key 的相似度：
$$
W_{score} = Q @ K^T
$$
- 输入 \(Q\) 维度：`(8, 5, 96)`
- 输入 \(K^T\) 维度：`(8, 96, 5)`
- 输出 \(W_{score}\) 维度：`(8, 5, 5)`

---

### 4. 计算多头隐藏状态
用注意力得分对 Value 进行加权聚合：
$$
H = W_{score} @ V
$$
- 输入 \(W_{score}\) 维度：`(8, 5, 5)`
- 输入 \(V\) 维度：`(8, 5, 96)`
- 输出 \(H\) 维度：`(8, 5, 96)`

---

### 5. 合并多头输出
将所有头的输出拼接并恢复为原始维度：
$$
H = H.\text{reshape}(\text{len}, \text{hidden})
$$
- 输入 \(H\) 维度：`(8, 5, 96)`
- 输出 \(H\) 维度：`(5, 768)`（与输入维度一致）

## 🎯 为什么用多头？
- 单个注意力头只能学习单一模式的语义关联。
- 多头机制让模型可以**并行捕捉多种不同的语义关联**（如主谓、动宾关系），获得更丰富的上下文信息，提升理解能力。

## 🔄 形状变换的作用
1.  **拆分（多头化）**：将高维向量 `(len, hidden)` 拆分为 `(head, len, hidden/head)`，让每个头独立计算，提升效率。
    *   例：`(5, 768)` → 拆分为 8 个头 → `(8, 5, 96)`
2.  **合并（恢复维度）**：将所有头的输出拼接回 `(len, hidden)` 维度，保证后续网络层输入维度一致。
    *   例：`(8, 5, 96)` → 合并 → `(5, 768)`

# 多层注意力机制
多层注意力机制是将**多头自注意力层与前馈网络层堆叠**形成的层级结构，常见于 Transformer 类模型。

1.  **层级堆叠逻辑**
    - 每层均以「多头自注意力 + 残差连接 + 层归一化」为核心单元，叠加前馈网络子层，逐层提炼输入序列的语义信息。
    - 下层输出作为上层输入，浅层捕捉基础词法关联，深层建模复杂句法、语义依赖。

2.  **核心优势**
    - 残差连接与归一化机制保障深层网络稳定训练，避免梯度消失。
# ⚡ 前馈神经网络（FFN）

---

## 1. 核心定位
FFN 是 Transformer 编码器/解码器层的**核心组件**，紧跟在多头自注意力层之后，负责对注意力输出进行深度非线性特征变换。

## 2. 计算公式
\[
H_{FNN} = \text{Relu}\left(H_{Attn} @ W_1 + B_1\right) @ W_2 + B_2
\]
- **输入**：\(H_{Attn}\)（多头自注意力层的输出，维度 \((1, 768)\)）
- **输出**：\(H_{FNN}\)（与输入维度一致，为 \((1, 768)\)）

## 3. 公式拆解
1.  **升维变换**：\(H_{Attn} @ W_1 + B_1\)
    - 通过权重矩阵 \(W_1\)（\(768 \times 2048\)）将输入从 768 维映射到 2048 维，偏置 \(B_1\) 增加灵活性。
    - 目的：将特征投射到高维空间，进行更充分的特征交互。
2.  **非线性激活**：\(\text{Relu}(\dots)\)
    - 对升维后的结果应用 ReLU 激活函数，将负数置为 0，保留正数。
    - 目的：引入非线性，让模型学习复杂的语义模式。
3.  **降维变换**：\(\dots @ W_2 + B_2\)
    - 通过权重矩阵 \(W_2\)（\(2048 \times 768\)）将特征从 2048 维映射回 768 维，偏置 \(B_2\) 调整输出。
    - 目的：将高维空间学到的复杂特征压缩回原维度，适配后续网络层。

## 4. 在 Transformer 中的作用
- **非线性建模**：对自注意力层捕捉到的全局关联信息进行深度加工，学习更复杂的语义模式。
- **维度升降维**：高维空间增强特征表达能力，降维后保持维度一致性，支持残差连接。
- **与注意力层互补**：注意力层负责“全局关联”，FFN 负责“逐位置”独立计算，结合后提升模型的全局与局部特征处理能力。

## 5. 维度变化
1.  输入：\(H_{Attn}\) → \((1, 768)\)
2.  升维：\(H_{Attn} @ W_1 + B_1\) → \((1, 2048)\)
3.  激活：ReLU → \((1, 2048)\)
4.  降维：\(\text{ReLU输出} @ W_2 + B_2\) → \((1, 768)\)


# 🧩 Transformer 注意力块中的残差连接 + 层归一化
这是 Transformer 编码器/解码器层的标准结构，它通过残差连接和层归一化的组合，确保深层网络能够稳定训练。

---
## 🔄 完整计算流程
### 1. 多头自注意力（MHA）子层
1.  **计算注意力输出**
    \[
    MHA_{output} = \text{MultiHeadAttention}(\text{Input})
    \]
2.  **残差连接 1**
    \[
    ResNet_{output1} = \text{Input} + MHA_{output}
    \]
    直接将输入与注意力输出相加，保留原始信息，缓解梯度消失。
3.  **层归一化 1**
    \[
    LayerNorm_{output1} = LN(ResNet_{output1})
    \]
    对残差连接的输出做归一化，稳定训练过程，防止梯度爆炸。

---

### 2. 前馈网络（FFN）子层
1.  **计算前馈输出**
    \[
    FNN_{output} = \text{FeedForward}(LayerNorm_{output1})
    \]
2.  **残差连接 2**
    \[
    ResNet_{output2} = LayerNorm_{output1} + FNN_{output}
    \]
    将归一化后的注意力输出与前馈输出相加，保留子层输入信息。
3.  **层归一化 2**
    \[
    LayerNorm_{output2} = LN(ResNet_{output2})
    \]
    对前馈子层的残差输出做归一化，得到整个注意力块的最终输出。

---

## ⚠️ 为何不使用批次归一化（BatchNorm）？
- **批次归一化**是对一个批次内所有样本的同一特征维度做归一化，依赖于批次内的统计信息。
- **层归一化**是对单个样本的所有特征维度做归一化，**仅使用当前时间步的特征值**，不依赖批次统计。
- Transformer 处理的是变长序列，不同样本的序列长度不同，无法在批次维度上稳定计算均值和方差，因此层归一化是更合适的选择。

---

## 🎯 核心设计思想
- **残差连接（ResNet）**：为每个子层提供一条“信息捷径”，确保梯度可以顺利回传，避免深层网络的退化问题。
- **层归一化（LayerNorm）**：在每个残差连接之后对特征做归一化，使每层的输入分布更稳定，加速模型收敛。
- **组合作用**：这种“子层计算 → 残差连接 → 层归一化”的模式，是 Transformer 能够训练上百层深度网络的关键。


# 🧮 归一化与 Softmax 模块整合笔记

---

## 一、归一化的两种核心方式
### 1. 线性归一化
- **输入**：原始得分向量 \([\hat{y}_1, \hat{y}_2, \hat{y}_3, \hat{y}_4, \hat{y}_5]\)
- **公式**：
\[
\text{LinearNorm}_i = \frac{\hat{y}_i}{\hat{y}_{\text{sum}}}, \quad \hat{y}_{\text{sum}} = \hat{y}_1 + \hat{y}_2 + \hat{y}_3 + \hat{y}_4 + \hat{y}_5
\]
- **输出形态**：
\[
\begin{bmatrix}\frac{\hat{y}_1}{\hat{y}_{\text{sum}}}, \frac{\hat{y}_2}{\hat{y}_{\text{sum}}}, \frac{\hat{y}_3}{\hat{y}_{\text{sum}}}, \frac{\hat{y}_4}{\hat{y}_{\text{sum}}}, \frac{\hat{y}_5}{\hat{y}_{\text{sum}}}\end{bmatrix}
\]
- **特点**：直接算术求和归一化，若原始得分含负数则输出可能为负，不满足概率定义。
- **适用场景**：仅用于非负得分的简单归一化，不适用于分类或注意力权重计算。

### 2. Softmax 归一化
- **输入**：原始得分向量 \([\hat{y}_1, \hat{y}_2, \hat{y}_3, \hat{y}_4, \hat{y}_5]\)
- **公式**：
\[
\text{Softmax}_i = \frac{e^{\hat{y}_i}}{e^{\hat{y}_{\text{sum}}}}, \quad e^{\hat{y}_{\text{sum}}} = e^{\hat{y}_1} + e^{\hat{y}_2} + e^{\hat{y}_3} + e^{\hat{y}_4} + e^{\hat{y}_5}
\]
- **输出形态**：
\[
\begin{bmatrix}\frac{e^{\hat{y}_1}}{e^{\hat{y}_{\text{sum}}}}, \frac{e^{\hat{y}_2}}{e^{\hat{y}_{\text{sum}}}}, \frac{e^{\hat{y}_3}}{e^{\hat{y}_{\text{sum}}}}, \frac{e^{\hat{y}_4}}{e^{\hat{y}_{\text{sum}}}}, \frac{e^{\hat{y}_5}}{e^{\hat{y}_{\text{sum}}}}\end{bmatrix}
\]
- **特点**：先指数化（转为正数）再归一化，输出范围 \([0, 1]\) 且和为 1，符合概率分布。
- **适用场景**：多分类任务输出层、注意力权重计算等需要概率解释的场景。

---

## 二、注意力权重的 Softmax 计算流程（Transformer 标准实现）
1.  **第一步：计算原始注意力得分**
\[
W_{\text{score}} = Q @ K^T
\]
    计算 Query 与 Key 的相似度，得到原始得分矩阵。

2.  **第二步：直接应用 Softmax（未缩放）**
\[
W_{\text{score}} = \text{softmax}(Q @ K^T)
\]
    将原始得分转化为概率分布，但当 `hidden_size` 较大时，得分易进入 Softmax 饱和区，导致梯度消失。

3.  **第三步：加入缩放因子的标准实现**
\[
W_{\text{score}} = \text{softmax}\left(\frac{Q @ K^T}{\sqrt{\text{hidden\_size}}}\right)
\]
    用 \(\sqrt{\text{hidden\_size}}\) 缩放得分，稳定数值与梯度，这是 Transformer 中的最终公式。
